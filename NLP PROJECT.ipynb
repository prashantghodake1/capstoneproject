{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f364d30",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967b1d8",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Hospitality reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML = Predicting Hotels Booking Demands. / Predictions of hotel bookings cancellation.\n",
    "Time series =Sales Forecasting of Agricultural Equipment.\n",
    "NLP = Sentiment Analysis of Hospitality reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCR:\n",
    "    Sentiment-based binary classification model (positive/ negative) for customer reviews received on businessâ€™ \n",
    "    facebook page and twitter handling for Idea is to build an inhouse customer support team,to resolve customer issues\n",
    "    through phone call,ensuring they revisit and immprove the service facility ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfc17d",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "They provided the data on their server and our team has fetched this data to our server \n",
    "Iinitially data was so noisy ( it contained special characteres ) and target feature is in positive(1) and negative(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "they provided near about 200000 reviws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845f2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ed2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Oracle\\Downloads\\a1_RestaurantReviews_HistoricDump.tsv', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a891fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24313c18",
   "metadata": {},
   "source": [
    "### Text Wrangling and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf90624",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9902589",
   "metadata": {},
   "outputs": [],
   "source": [
    "we check for null values in data and we found some samples so we drop this samples from data.\n",
    "then we convert the target from Positive and negative to 1 and 0 by using replacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2cf9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review    0\n",
       "Liked     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39155ca0",
   "metadata": {},
   "source": [
    "#### Text Wrangling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94a596db",
   "metadata": {},
   "source": [
    "Tokenisation : in tokenisation we create list of words from pararaph or list of sentences from paragraph.\n",
    "    we used wordtokenisation for our model.\n",
    "    once we done with tokenisation we go for text cleaning \n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "Cleaning:\n",
    "    First we need to remove all the unnessary charater from reviews for eg. queation mark , comma ,number ect. \n",
    "    we want only alphabate to dectect the review is positive or no\n",
    "    x.isalpha() or regular expression\n",
    "    \n",
    "Normalisation:\n",
    "    if there same words but in different case(upper and lower) then our model consider it as seprate words so it increases\n",
    "    number of features in model and also it divide there weightage.\n",
    "    so in this step we convert all the text in same case (lower case)\n",
    "    \n",
    "Stop words Removal:\n",
    "    in this we remove all the english stop words which are imp for makes sentence meaningful. ex = is are of there etc\n",
    "    so here we remove all the stopwords of english.\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    after this step we get the only important words without special characters and stopwords\n",
    "    \n",
    "Stemming or Lemmitizig :\n",
    "    in this step we take words into their root form (it groups the simmillar words and make coomon root word).\n",
    "    Ex-cleaned ,cleaning, clean ,cleaniless = clean(rootword)\n",
    "    it helps to avoid model from curse of dimensionality/High dimensionality.\n",
    "1)stemming:\n",
    "    it is aggresive pruner which pruns the words into its root form with or without meaning.\n",
    "    ex=(difference,differencing,differenciate,differ,different) = differ\n",
    "\n",
    "2)Lemmitization:\n",
    "    it converts the group of words into the root form with meaning.\n",
    "    ex=(difference,differencing,differenciate,differ,different) = different\n",
    "    \n",
    "    from nltk.stem import LancasterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b3a64",
   "metadata": {},
   "source": [
    "### Vectrization  (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ae277",
   "metadata": {},
   "outputs": [],
   "source": [
    "before applying the vectorization we join the list of words into strings.\n",
    "by using join function\n",
    "\n",
    "Vectorization:\n",
    "    vectrization is nothing but the table of frequency of words representation.\n",
    "    it converts text to numbers.\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "1) Count-Vectroization:\n",
    "    it returns the count of word present in respective sentense.\n",
    "    its weightage in integer\n",
    "    other than max_features words they are stored into out of vocabualary(OOV)\n",
    "    \n",
    "    cv = CountVectorizer(lowercase=True ,stop_words = 'english',max_df=.95,max_features=1000)\n",
    "    in this we can controll the numbers of feature required and we can drop the domain specific stop words like waiter,meal\n",
    "\n",
    "2) Tf-idf (Term Frequency - inverse document frequency):\n",
    "    it returns the value of words in term of percentage it is in float(0-1)\n",
    "    tf = TfidfVectorizer(lowercase = True,stop_words='english',max_df=0.95,max_features=1000)\n",
    "    tf = tf*log(idf)\n",
    "    \n",
    "for self = It returns the output in sparx matrix,we have to convert the sparx matrix into array for model building.\n",
    "    to convert it into array = feature_name.A\n",
    "    \n",
    "if we convert this sparx array into dataframe we get the Word requency table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863ecd4",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5411bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "We split the data into(train and test) x_train ,x_test,y_train ,y_test\n",
    "\n",
    "we build model using Bernoulii naive bayes algorithm and SVM algoritham.\n",
    "once model got trained we checked for the prediction on the testing dataset.\n",
    "\n",
    "we got the accuracy on SVM 85 % data.\n",
    "and on Naive bayes 89 %\n",
    "\n",
    "\n",
    "we get best model on using Lemmitization and tfidf vectorization and naive bayes model.\n",
    "\n",
    "for evalution we check accuracy score and confusion matrix\n",
    "\n",
    "In this we gave importance to Precision(FP) as we can not tolerate negative sentiments which are very important for \n",
    "the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes (CountVectorizer)\n",
    "    test data :  0.8756674294431731\n",
    "    train data :  0.8777417509059698\n",
    "        \n",
    "SVM (CountVectorizer)\n",
    "    test data :  0.8891431477243834\n",
    "    train data :  0.9528259902091678\n",
    "        \n",
    "TF-IDF (Naive Baiyes)\n",
    "    test data :  0.8591406051360285\n",
    "    train data :  0.854536207006167\n",
    "    \n",
    "TF-IDF (SVM)\n",
    "    test data :  0.8906687007373506\n",
    "    train data :  0.962934706592917"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
